# -*- coding: utf-8 -*-
"""3D Metal Printing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NoeQ_QcZDqvSMcFJfLt6MP-wxTcwmQPz
"""

# created by: Luke McDaniel
# institution: California Polytechnic State University
# course : CPE 480 Artificial Intelligence
# description: This program is an image classifier class
# calling the two_layer_predicttion function after training the
# model can be used to classify an image as error or no/error,
# if the image has an error the program will classify the error.

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img


import pathlib
import zipfile
import urllib.request

from google.colab import files

# class used to bundle all image classification routines
class printLayerClassifier:

  def __init__(self, binary_epochs, error_epochs, num_error_classes):
    self.binary_epochs = binary_epochs
    self.error_epochs = error_epochs
    self.num_error_classes = num_error_classes
    self.batch_size = 32
    self.img_height = 128
    self.img_width = 140
    self.img_size = (self.img_height, self.img_width)
    self.binary_model = self.build_binary_model(input_shape=self.img_size + (3,))
    self.error_model = self.build_error_model(input_shape=self.img_size + (3,), num_classes=self.num_error_classes)

  # use this function to create a path to the dataset used by the binary model
  # current code downloads dataset from github, change if new dataset is created or more images are added
  # inside of main directory there should be two sub directories, Error and No Error
  def get_binary_model_dataset(self):
    err_noerr_dataset_url = "https://github.com/lmcdaniel1998/3D_Metal_Printing/blob/main/Err_NoErr.tar.gz?raw=true"
    err_noerr_data_dir = tf.keras.utils.get_file(origin=err_noerr_dataset_url, 
                                      fname='Err_NoErr', 
                                      untar=True)
    err_noerr_data_dir = pathlib.Path(err_noerr_data_dir)

    image_count = len(list(err_noerr_data_dir.glob('*/*.jpg')))
    print(image_count)
    return err_noerr_data_dir

  # use this function to create a path to the dataset used by the error classification model
  # current code downloads dataset from github, change if new dataset is created or more images are added
  # inside of main directory there should be the same number of sub directories as Error classes
  def get_error_model_dataset(self):
    err_dataset_url = "https://github.com/lmcdaniel1998/3D_Metal_Printing/blob/main/Error_Classes.tar.gz?raw=true"
    err_data_dir = tf.keras.utils.get_file(origin=err_dataset_url, 
                                      fname='Error_Classes', 
                                      untar=True)
    err_data_dir = pathlib.Path(err_data_dir)

    image_count = len(list(err_data_dir.glob('*/*.jpg')))
    print(image_count)
    return err_data_dir

  # uses 80% of dataset to create a training dataset
  def create_training_dataset(self, dataset_dir):
    train_ds = tf.keras.preprocessing.image_dataset_from_directory(
      dataset_dir,
      validation_split=0.2,
      subset="training",
      seed=123,
      image_size=(self.img_height, self.img_width),
      batch_size=self.batch_size)
    return train_ds

  # use 20% of dataset to create a validation dataset
  def create_val_dataset(self, dataset_dir):
    val_ds = tf.keras.preprocessing.image_dataset_from_directory(
      dataset_dir,
      validation_split=0.2,
      subset="validation",
      seed=123,
      image_size=(self.img_height, self.img_width),
      batch_size=self.batch_size)
    return val_ds

  # used to generate list of class names in dataset based off of sub directory names
  # needs to be called for classification output
  def get_class_names(self, train_ds):
    class_names = train_ds.class_names
    print(class_names)
    return class_names

  # imports binary and error datasets, separates datasets and creates training and validation sets
  def prepare_datasets(self):
    # import datasets
    err_noerr_data_dir = self.get_binary_model_dataset()
    err_data_dir = self.get_error_model_dataset()

    # binary datasets and classes
    self.err_noerr_train_ds = self.create_training_dataset(err_noerr_data_dir)
    self.err_noerr_val_ds = self.create_val_dataset(err_noerr_data_dir)
    self.binary_class_names = self.get_class_names(self.err_noerr_train_ds)

    # error datasets and classes
    self.err_train_ds = self.create_training_dataset(err_data_dir)
    self.err_val_ds = self.create_val_dataset(err_data_dir)
    self.error_class_names = self.get_class_names(self.err_train_ds)


  def define_data_augmentation(self):
    data_augmentation = keras.Sequential(
        [
        layers.experimental.preprocessing.RandomFlip("horizontal"),
        layers.experimental.preprocessing.Normalization(),
        layers.experimental.preprocessing.RandomContrast(0.25)
        ]
    )
    return data_augmentation

  def build_binary_model(self, input_shape):
    # load in data augmentation parameters
    data_augmentation = self.define_data_augmentation()
    inputs = keras.Input(shape=input_shape)         # 640 X 700 for 3D Metal Printer but scaled down by a factor of 5
    # data augmentation will occur in model
    # this means data augmentation will occur on the device, synchronously with the
    # rest of the model execution. IMPROVES PERFORMANCE if training on GPU
    model = data_augmentation(inputs)
    model = layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape))(model)
    model = layers.Conv2D(16, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Conv2D(32, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Conv2D(64, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Flatten()(model)
    model = layers.Dense(128, activation='relu')(model)
    outputs = layers.Dense(2)(model)
    return keras.Model(inputs, outputs)

  def build_error_model(self, input_shape, num_classes):
    # load in data augmentation parameters
    data_augmentation = self.define_data_augmentation()
    inputs = keras.Input(shape=input_shape)         # 640 X 700 for 3D Metal Printer but scaled down by a factor of 5
    # data augmentation will occur in model
    # this means data augmentation will occur on the device, synchronously with the
    # rest of the model execution. IMPROVES PERFORMANCE if training on GPU
    model = data_augmentation(inputs)
    model = layers.experimental.preprocessing.Rescaling(1./255, input_shape=(input_shape))(model)
    model = layers.Conv2D(16, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Conv2D(32, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Conv2D(64, 3, padding='same', activation='relu')(model)
    model = layers.MaxPooling2D()(model)
    model = layers.Flatten()(model)
    model = layers.Dense(128, activation='relu')(model)
    outputs = layers.Dense(num_classes)(model)
    return keras.Model(inputs, outputs)

  # this function trains the binary classification model
  def train_binary_model(self):
    # load in data augmentation parameters
    data_augmentation = self.define_data_augmentation()
    # compile the model using accuracy as a measurement metric
    self.binary_model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    # train the model over the defined epochs
    binary_history = self.binary_model.fit(
      self.err_noerr_train_ds,
      validation_data=self.err_noerr_val_ds,
      epochs=self.binary_epochs,
      verbose=0
    )
    self.binary_history = binary_history

  # this function trains the error classification model
  def train_error_model(self):
    # load in data augmentation parameters
    data_augmentation = self.define_data_augmentation()
    # compile the model using accuracy as a measurement metric
    self.error_model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    # train the model over the defined epochs
    error_history = self.error_model.fit(
      self.err_train_ds,
      validation_data=self.err_val_ds,
      epochs=self.error_epochs,
      verbose=0
    )
    self.error_history = error_history

  # call this function to train both models
  def train_models(self):
    self.train_binary_model()
    self.train_error_model()

  # image will first be passed through the binary model to detect if there is an error,
  # if there is an error the image will be passed through the error model to determine
  # what kind of error occured
  # call this function whenever you want to classify an image !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  def two_layer_prediction(self, img):
    # get error / no error prediction
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)                    # create a batch

    binary_predictions = self.binary_model.predict(img_array)
    binary_score = tf.nn.softmax(binary_predictions[0])

    # must be greater than 85% confidence that image has error
    if (self.binary_class_names[np.argmax(binary_score)] == 'A1_ERROR' and
        (100 * np.max(binary_score)) >= 85.0):
      # classify error
      error_predictions = self.error_model.predict(img_array)
      error_score = tf.nn.softmax(error_predictions[0])
      print(
        "Error, most likely {} percent conficence: {:.2f}"
        .format(self.error_class_names[np.argmax(error_score)], 100 * np.max(error_score))
      )

    else:
      print(
        "No Error, percent confidence: {:.2f}"
        .format(100 * np.max(binary_score))
      )

  def test(self):
    # get test images from github
    test_data_url = "https://github.com/lmcdaniel1998/3D_Metal_Printing/blob/main/Test_Images.tar.gz?raw=true"
    test_data_dir = tf.keras.utils.get_file(origin=test_data_url, 
                                      fname='Test_Images', 
                                      untar=True)
    test_data_dir = pathlib.Path(test_data_dir)

    image_count = len(list(test_data_dir.glob('*/*.jpg')))

    # place all test images into a list
    test_images = []
    # Debris error
    DebrisError1 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Debris/0044_1Coupon_1_hollowcube_00113.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(DebrisError1)

    DebrisError2 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Debris/0010_1halfoctahedron_1seveneightoctahedron_1doublebell_00475.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(DebrisError2)

    # Uneven Powder error
    UnevenPowderError1 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Uneven_Powder/02-17-2017_3_GleebleXY_3_Horse_00379.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(UnevenPowderError1)

    UnevenPowderError2 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Uneven_Powder/0004_Baskett Thesis Validation_14overhangs_00191.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(UnevenPowderError2)

    # Recoater Skipping error
    RecoaterSkippingError1 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Recoater_Skipping/0064_6AFM_5LargeAFM_00197.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(RecoaterSkippingError1)

    RecoaterSkippingError2 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Recoater_Skipping/0010_1halfoctahedron_1seveneightoctahedron_1doublebell_00511.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(RecoaterSkippingError2)

    # Recoater Streaking error
    RecoaterStreakingError1 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Recoater_Streaking/0041_1cube_3hex_00173.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(RecoaterStreakingError1)

    RecoaterStreakingError2 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Recoater_Streaking/0054_1Ring_5BigLattice_00423.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(RecoaterStreakingError2)

    # OK Layer
    OkLayer1 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Ok_Layers/0056_h15_h40_h25_00105.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(OkLayer1)

    # Ok Layer
    OkLayer2 = keras.preprocessing.image.load_img(
        '/root/.keras/datasets/Test_Images/Ok_Layers/0003__2lattice_5tensile_2tetraballs_4thindisk_00758.jpg', target_size=(self.img_height, self.img_width)
    )
    test_images.append(OkLayer2)

    correct_classes = ['Debris Error', 'Debris Error', 'Uneven Powder Error', 'Uneven Powder Error', 'Recoater Skipping Error',
                       'Recoater Skipping Error', 'Recoater Streaking Error', 'Recoater Streaking Error', 'No Error', 'No Error']
    
    print("Correct Classifications: ")
    for img in correct_classes:
      print(img)
    print("Algorithm Classifications: ")
    for img in test_images:
      self.two_layer_prediction(img)


def simple_test():
  # order of operation for this class

  # initialize classifier
  classifier = printLayerClassifier(60, 70, 4)

  # create datasets
  classifier.prepare_datasets()

  # train binary model
  classifier.train_models()

  # can start classifying images after models are trained

  # test
  classifier.test()

simple_test()